{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOR7/NFDqnNAnuo1W9Y+F6Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maayanorner/RL_snippets/blob/main/colabs/rl_lib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ray version without device bugs\n",
        "!pip install -U ray[default]==2.0 > /dev/null 2>&1\n",
        "!pip install -U ray[rllib]==2.0 > /dev/null 2>&1\n",
        "# Install other dependencies\n",
        "!pip install pygame > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install -U colabgymrender > /dev/null 2>&1\n",
        "!pip install imageio==2.4.1 > /dev/null 2>&1\n",
        "\n",
        "# Install gym\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install gym[accept-rom-license] > /dev/null 2>&1\n",
        "#!pip install gym[accept-rom-license]\n",
        "\n",
        "# GPU monitoring\n",
        "!pip install GPUtil > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "e82I_fpH9qdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['SDL_VIDEODRIVER']='dummy'\n",
        "import pygame\n",
        "pygame.display.set_mode((640,480))"
      ],
      "metadata": {
        "id": "73cl9ovlbHti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "from ray import air, tune\n",
        "\n",
        "ray.init()"
      ],
      "metadata": {
        "id": "Ik-dA3GG-IMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym import envs\n",
        "import gym\n",
        "#print(envs.registry.all())"
      ],
      "metadata": {
        "id": "gnptQjV4cgZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = \"MsPacman-v4\"\n",
        "from ray.rllib.env.wrappers.atari_wrappers import WarpFrame, MaxAndSkipEnv, FrameStack\n",
        "dim = 84\n",
        "\n",
        "class AtariWapped(gym.Env):\n",
        "    def __init__(self, env_config={}):\n",
        "        self.env = gym.make(env_name, **env_config)\n",
        "        self.env = WarpFrame(self.env, dim=dim)\n",
        "        self.env = MaxAndSkipEnv(self.env, skip=4)\n",
        "        self.env = FrameStack(self.env, k=4)\n",
        "        #env = gym.make(env_name, frameskip=0)\n",
        "        #self.env = gym.wrappers.AtariPreprocessing(env, noop_max=30, frame_skip=16, screen_size=dim, terminal_on_life_loss=True, grayscale_obs=True, grayscale_newaxis=False, scale_obs=False)\n",
        "        self.action_space = self.env.action_space\n",
        "        self.observation_space = self.env.observation_space\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        return self.env.reset()\n",
        "\n",
        "    def render(self, *args, **kwargs):\n",
        "        return self.env.render(*args, **kwargs)\n",
        "\n",
        "e_info =  AtariWapped()\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "config = deepcopy({\n",
        "  'observation_space': e_info.observation_space,\n",
        "  'action_space': e_info.action_space\n",
        "})\n",
        "\n",
        "wrapped_name = AtariWapped\n",
        "from ray.tune.registry import register_env\n",
        "\n",
        "def env_creator(env_config):\n",
        "    return AtariWapped(env_config)\n",
        "final_env_name = \"attari_wrapped\"\n",
        "register_env(final_env_name, env_creator)\n"
      ],
      "metadata": {
        "id": "Zn4g_Ve6W0s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AtariWapped().reset().shape"
      ],
      "metadata": {
        "id": "LtwSjIpmqwev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ray.rllib.algorithms.dqn import DQN\n",
        "from ray.rllib.algorithms.ppo import PPO\n",
        "from math import sqrt"
      ],
      "metadata": {
        "id": "NQr0oNXK16qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ray.tune.stopper import Stopper, TrialPlateauStopper, CombinedStopper, MaximumIterationStopper\n",
        "\n",
        "\n",
        "# Stop on degradation\n",
        "es = CombinedStopper(TrialPlateauStopper(metric='episode_reward_mean', num_results = 4, mode = 'max', grace_period = 4), MaximumIterationStopper(max_iter=2000))"
      ],
      "metadata": {
        "id": "gE8yDsUj5Ele"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ray.tune.schedulers import HyperBandScheduler\n",
        "\n",
        "sc = HyperBandScheduler('time_total_s', metric='episode_reward_mean', max_t=1800, mode=\"max\")"
      ],
      "metadata": {
        "id": "ra0LsrJYrG9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "\n",
        "eps_n_workes = 0.001"
      ],
      "metadata": {
        "id": "O8IsH8rSsy48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LYl6DJz88HT"
      },
      "outputs": [],
      "source": [
        "Algo = DQN\n",
        "save_per_training_iteration = 10\n",
        "train_batch_size = 512\n",
        "num_workers = 30#100\n",
        "num_av_cpus = psutil.cpu_count() - 1\n",
        "\n",
        "# run on CPU\n",
        "workers_gpu_frac = 0\n",
        "num_gpus_per_worker = (1/num_workers)*workers_gpu_frac \n",
        "\n",
        "lr = [1e-4] #[1e-5, 1e-4]#[0.01, 1e-3, 1e-4, 1e-5]\n",
        "config = {\n",
        "        \"env\": final_env_name,\n",
        "        \"num_gpus\": 1-workers_gpu_frac,\n",
        "        \"num_gpus_per_worker\": num_gpus_per_worker,#(1/num_workers)*workers_gpu_frac,\n",
        "        \"num_cpus_per_worker\": (1/num_workers)*num_av_cpus, \n",
        "        \"num_workers\": num_workers,\n",
        "        \"lr\": tune.grid_search(lr),\n",
        "        \"framework\": \"torch\",\n",
        "        #'evaluation_num_workers': 0,\n",
        "        #'evaluation_parallel_to_training': False,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        #'num_rollout_workers': 1,\n",
        "        'soft_horizon': False,\n",
        "        'horizon': 100000,\n",
        "  }\n",
        "\n",
        "# if type(Algo) == DQN:\n",
        "#   config[\"optimizer\"] = \"ADAM\"\n",
        "\n",
        "if type(Algo) == PPO:\n",
        "  # Like in paper\n",
        "  config['grad_clip'] = 0.2\n",
        "  config['horizon'] = 128\n",
        "  config[\"optimizer\"] = \"ADAM\"\n",
        "  #config[\"train_batch_size\"] = 32*num_workers\n",
        "  lr = [1e-4*sqrt(num_workers)]\n",
        "  \n",
        "\n",
        "results = tune.Tuner(\n",
        "    Algo,\n",
        "    tune_config=tune.TuneConfig(scheduler=sc),\n",
        "    run_config=air.RunConfig(\n",
        "        #stop=es,#{\"training_iteration\": training_iteration, },\n",
        "        checkpoint_config=air.CheckpointConfig(checkpoint_frequency=save_per_training_iteration),\n",
        "        name=\"example-experiment-atari\",\n",
        "        local_dir=\"./example-experiment\"\n",
        "        ),\n",
        "    param_space=config,\n",
        ").fit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IGo7kM9TO6jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ray.tune import ExperimentAnalysis\n",
        "analysis = ExperimentAnalysis(\"/content/example-experiment/\")"
      ],
      "metadata": {
        "id": "gFDlI_3jAZwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial_logdir = analysis.get_best_logdir(metric=\"episode_reward_mean\", mode=\"max\")  # Can also just specify trial dir directly\n",
        "\n",
        "checkpoints = analysis.get_trial_checkpoints_paths(trial_logdir)  # Returns tuples of (logdir, metric)\n",
        "best_checkpoint = analysis.get_best_checkpoint(trial_logdir, metric=\"episode_reward_mean\", mode=\"max\")"
      ],
      "metadata": {
        "id": "pT9Yz1g0UCMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Algo(config={\"framework\": \"torch\"}, env=final_env_name)\n",
        "agent.restore(best_checkpoint)"
      ],
      "metadata": {
        "id": "nba--uN5VDGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from colabgymrender.recorder import Recorder\n",
        "\n",
        "video_every = 1\n",
        "env = AtariWapped()\n",
        "env = Recorder(env, \"./video\", 0.2)"
      ],
      "metadata": {
        "id": "O2zENpLgVc5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run until episode ends\n",
        "episode_reward = 0\n",
        "done = False\n",
        "obs = env.reset()\n",
        "while not done:\n",
        "    action = agent.compute_action(obs)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    episode_reward += reward\n",
        "\n",
        "print(episode_reward)"
      ],
      "metadata": {
        "id": "ewTjovbHFVff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pa0M2XqmYqDh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}